# websucker Quickstart

**websucker** takes websites, recursively scans all their subpages, and outputs each subpage to a text file and/or summarizes each subpage. This tool should not be used on sites requesting crawling exclusion on their `robots.txt`.

## TODO

- [ ] intelligent crawling (specify goal. only output if target information found)
